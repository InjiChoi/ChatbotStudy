{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 37.4 MB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz in /home/injichoi/.local/lib/python3.8/site-packages\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.0.0-py3-none-any.whl size=37405963 sha256=97bb5c1128ccdc685fb13804783f5407398b57c06304e9575f92771461f5b601\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-t03zgxcy/wheels/11/b2/a7/d98f4f5d4d81e6859e30ad2c8928decde3e7077f1fd8366ac0\n",
      "Successfully built en-core-web-sm\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/injichoi/.local/lib/python3.8/site-packages/en_core_web_sm -->\n",
      "    /home/injichoi/.local/lib/python3.8/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/injichoi/.local/lib/python3.8/site-packages (2.0.11)\n",
      "Requirement already satisfied: numpy>=1.7 in /home/injichoi/.local/lib/python3.8/site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /home/injichoi/.local/lib/python3.8/site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /home/injichoi/.local/lib/python3.8/site-packages (from spacy) (1.31.2)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /home/injichoi/.local/lib/python3.8/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.1 in /home/injichoi/.local/lib/python3.8/site-packages (from spacy) (6.10.3)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/injichoi/.local/lib/python3.8/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: pathlib in /home/injichoi/.local/lib/python3.8/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: ujson>=1.35 in /home/injichoi/.local/lib/python3.8/site-packages (from spacy) (4.3.0)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /home/injichoi/.local/lib/python3.8/site-packages (from spacy) (0.2.9)\n",
      "Requirement already satisfied: regex==2017.4.5 in /home/injichoi/.local/lib/python3.8/site-packages (from spacy) (2017.4.5)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /home/injichoi/.local/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.1->spacy) (0.6.2)\n",
      "Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in /home/injichoi/.local/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.1->spacy) (0.4.7.1)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /home/injichoi/.local/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.1->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /home/injichoi/.local/lib/python3.8/site-packages (from thinc<6.11.0,>=6.10.1->spacy) (1.10.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (4.51.0)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/lib/python3/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (1.14.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.1->spacy) (0.11.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3: No module named spacy\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am VERB\n",
      "learning VERB\n",
      "how ADV\n",
      "to PART\n",
      "build VERB\n",
      "chatbots NOUN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'I am learning how to build chatbots')\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am VERB\n",
      "going VERB\n",
      "to ADP\n",
      "London PROPN\n",
      "next ADJ\n",
      "week NOUN\n",
      "for ADP\n",
      "a DET\n",
      "meeting NOUN\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u'I am going to London next week for a meeting')\n",
    "for token in doc2:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google google PROPN\n",
      "release release NOUN\n",
      "\" \" PUNCT\n",
      "Move move PROPN\n",
      "Mirror mirror PROPN\n",
      "\" \" PUNCT\n",
      "AI ai PROPN\n",
      "experiment experiment NOUN\n",
      "that that ADJ\n",
      "matches match VERB\n",
      "your -PRON- ADJ\n",
      "pose pose NOUN\n",
      "from from ADP\n",
      "80,000 80,000 NUM\n",
      "images image NOUN\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u'Google release \"Move Mirror\" AI experiment that matches your pose from 80,000 images')\n",
    "for token in doc3:\n",
    "    print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- PRON PRP nsubj X True False\n",
      "am be VERB VBP aux xx True True\n",
      "learning learn VERB VBG ROOT xxxx True False\n",
      "how how ADV WRB advmod xxx True True\n",
      "to to PART TO aux xx True True\n",
      "build build VERB VB xcomp xxxx True False\n",
      "chatbots chatbot NOUN NNS dobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp('I am learning how to build chatbots')\n",
    "for token in doc4:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chuckle']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 표제어 추출\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "lemmatizer('chuckles', 'NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blaze']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('blazing', 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fast']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('fastest', 'ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Mountain View GPE\n",
      "California GPE\n",
      "109 CARDINAL\n",
      "US GPE\n"
     ]
    }
   ],
   "source": [
    "# 개체명 인식\n",
    "\n",
    "my_string = \"Google has its headquarters in Mountain View, California having revenue amounted to 109..65 villion US dollars\"\n",
    "doc = nlp(my_string)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Zuckerberg PERSON\n",
      "May 14, 1984 DATE\n",
      "New York GPE\n",
      "American NORP\n",
      "Facebook ORG\n"
     ]
    }
   ],
   "source": [
    "my_string2 = \"Mark Zuckerberg born May 14, 1984 in New York is an American technology entrepreneur and philanthropist best known for co-founding and leading Facebook as its chairman and CEO\"\n",
    "doc2 = nlp(my_string2)\n",
    "\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one', 'been', 'four', 'neither', 'around', 'beforehand', 'somehow', 'regarding', 'forty', 'whenever', 'sometime', 'always', 'down', 'until', 'that', 're', 'too', 'becomes', 'could', 'due', 'used', 'ourselves', 'but', 'this', 'any', 'bottom', 'former', 'per', 'amongst', 'already', 'behind', 'noone', 'rather', 'through', 'i', 'seem', 'beside', 'anywhere', 'can', 'various', 'which', 'whither', 'has', 'made', 'whole', 'ever', 'eleven', 'less', 'sometimes', 'whatever', 'also', 'many', 'there', 'cannot', 'amount', 'thus', 'us', 'above', 'wherever', 'your', 'twelve', 'eight', 'now', 'are', 'hence', 'up', 'who', 'by', 'without', 'onto', 'much', 'ca', 'get', 'otherwise', 'here', 'herein', 'ten', 'moreover', 'fifty', 'therein', 'those', 'yet', 'across', 'besides', 'except', 'namely', 'nevertheless', 'an', 'doing', 'last', 'each', 'upon', 'our', 'might', 'hers', 'hereupon', 'not', 'side', 'these', 'everywhere', 'is', 'seems', 'never', 'mine', 'back', 'do', 'own', 'top', 'two', 'thru', 'everyone', 'below', 'what', 'it', 'its', 'whereafter', 'even', 'third', 'else', 'himself', 'hereafter', 'five', 'along', 'during', 'at', 'being', 'both', 'first', 'front', 'seemed', 'than', 'since', 'after', 'throughout', 'name', 'call', 'then', 'meanwhile', 'something', 'full', 'me', 'toward', 'into', 'several', 'towards', 'have', 'must', 'where', 'thereupon', 'take', 'against', 'unless', 'very', 'within', 'anyone', 'part', 'so', 'somewhere', 'them', 'further', 'together', 'for', 'thereafter', 'nine', 'more', 'between', 'anyhow', 'hereby', 'thereby', 'therefore', 'mostly', 'put', 'seeming', 'would', 'either', 'whence', 'whom', 'yourself', 'did', 'none', 'quite', 'keep', 'or', 'although', 'with', 'sixty', 'my', 'latterly', 'him', 'make', 'enough', 'ours', 'latter', 'move', 'go', 'they', 'from', 'afterwards', 'becoming', 'no', 'she', 'wherein', 'to', 'nothing', 'just', 'other', 'next', 'does', 'fifteen', 'over', 'whose', 'herself', 'itself', 'three', 'whether', 'in', 'out', 'while', 'am', 'still', 'thence', 'beyond', 'once', 'the', 'why', 'often', 'six', 'themselves', 'whereas', 'please', 'few', 'yours', 'almost', 'his', 'if', 'using', 'how', 'hundred', 'may', 'myself', 'see', 'some', 'twenty', 'whoever', 'were', 'again', 'show', 'everything', 'most', 'had', 'a', 'such', 'though', 'we', 'formerly', 'on', 'nor', 'among', 'whereby', 'someone', 'well', 'be', 'done', 'indeed', 'should', 'nobody', 'anything', 'anyway', 'give', 'become', 'off', 'he', 'before', 'however', 'became', 'say', 'same', 'under', 'of', 'you', 'whereupon', 'about', 'will', 'perhaps', 'as', 'all', 'least', 'yourselves', 'via', 'serious', 'nowhere', 'others', 'every', 'only', 'empty', 'and', 'because', 'elsewhere', 'her', 'when', 'their', 'really', 'was', 'another', 'alone'}\n"
     ]
    }
   ],
   "source": [
    "# 불용어\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['is'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['hello'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[from, flight, Book]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 의존 구문 분석\n",
    "\n",
    "doc = nlp('Book me a flight from Bangalore to Goa')\n",
    "blr, goa = doc[5], doc[7]\n",
    "list(blr.ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[to, flight, Book]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(goa.ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[from, flight, Book]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc[5].ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[4].is_ancestor(doc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booking of table belongs to restaurant\n",
      "Booking of taxi belongs to hotel\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Book a table at the restaurant and the taxi to the hotel\")\n",
    "tasks = doc[2], doc[8] # table ,taxi\n",
    "tasks_target = doc[5], doc[11] # restaurant , hotel\n",
    "\n",
    "for task in tasks_target:\n",
    "    for tok in task.ancestors:\n",
    "        if tok in tasks:\n",
    "            print(\"Booking of {} belongs to {}\".format(tok, task))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-eeaac0fba9cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Book a table at the restaurant and the taxi to the hotel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdisplacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dep'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/displacy/__init__.py\u001b[0m in \u001b[0;36mserve\u001b[0;34m(docs, style, page, minify, options, manual, port)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mwsgiref\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimple_server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     render(docs, style=style, page=page, minify=minify, options=options,\n\u001b[0m\u001b[1;32m     59\u001b[0m            manual=manual)\n\u001b[1;32m     60\u001b[0m     \u001b[0mhttpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0.0.0.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/displacy/__init__.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(docs, style, page, minify, jupyter, options, manual)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmanual\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0m_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parsed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parsed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/displacy/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmanual\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0m_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parsed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parsed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/displacy/__init__.py\u001b[0m in \u001b[0;36mparse_deps\u001b[0;34m(orig_doc, options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGenerated\u001b[0m \u001b[0mdependency\u001b[0m \u001b[0mparse\u001b[0m \u001b[0mkeyed\u001b[0m \u001b[0mby\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0muser_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.to_bytes\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mto_bytes\u001b[0;34m(getters, exclude)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mserialized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmsgpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bin_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/msgpack_numpy.py\u001b[0m in \u001b[0;36mpackb\u001b[0;34m(o, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \"\"\"\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPacker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'encoding'"
     ]
    }
   ],
   "source": [
    "# 의존 구문 분석 인터랙티브 시각화\n",
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u'Book a table at the restaurant and the taxi to the hotel')\n",
    "displacy.serve(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning learning\n",
      "the code code\n",
      "messenger RNAs RNAs\n",
      "protein-coding potential potential\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Deep learning cracks the code of messenger RNAs and protein-coding potential')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How [-0.29742572  0.73939663 -0.04001489  0.44033998  2.8967493 ]\n",
      "are [-0.22302864 -1.6086986   1.0160725   0.9935979   0.29161382]\n",
      "you [ 0.19734347 -3.5988896   2.4843178   4.3797216   3.6640272 ]\n",
      "doing [-0.52638924 -2.6429667  -1.2825204   2.7562196   4.3926945 ]\n",
      "today [ 3.7530282  -1.1754489   3.604786    1.9008211   0.60974014]\n"
     ]
    }
   ],
   "source": [
    "# 유사도 확인\n",
    "doc = nlp('How are you doing today')\n",
    "for token in doc:\n",
    "    print(token.text, token.vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7879069618058205\n",
      "0.4193426346455411\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7646170387605933"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello_doc = nlp('hello')\n",
    "hi_doc = nlp('hi')\n",
    "hella_doc = nlp('hella')\n",
    "\n",
    "print(hello_doc.similarity(hi_doc))\n",
    "print(hello_doc.similarity(hella_doc))\n",
    "print('-------------------')\n",
    "\n",
    "GoT_str1 = nlp('When will next season of Game Thrones be releasing?')\n",
    "GoT_str2 = nlp('Game of Thrones next season release date?')\n",
    "GoT_str1.similarity(GoT_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word car is 100% similar to word car\n",
      "Word car is 71% similar to word truck\n",
      "Word car is 24% similar to word google\n",
      "Word truck is 71% similar to word car\n",
      "Word truck is 100% similar to word truck\n",
      "Word truck is 36% similar to word google\n",
      "Word google is 24% similar to word car\n",
      "Word google is 36% similar to word truck\n",
      "Word google is 100% similar to word google\n"
     ]
    }
   ],
   "source": [
    "example_doc = nlp('car truck google')\n",
    "\n",
    "for t1 in example_doc:\n",
    "    for t2 in example_doc:\n",
    "        similarity_perc = int(t1.similarity(t2)*100)\n",
    "        print(\"Word {} is {}% similar to word {}\".format(t1, similarity_perc, t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brexit\n",
      "is\n",
      "the\n",
      "impending\n",
      "withdrawal\n",
      "of\n",
      "the\n",
      "U.K.\n",
      "from\n",
      "the\n",
      "European\n",
      "Union\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Brexit is the impending withdrawal of the U.K. from the European Union.')\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_to pattern matched correctly. Printing values\n",
      "\n",
      "From:  Airport Station , To:  Hong Kong Station\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식\n",
    "\n",
    "import re\n",
    "\n",
    "sentence1 = 'Book me a metro from Airport Station to Hong Kong Station'\n",
    "sentence2 = 'Book me a cab from Hon Kong Airport to AsiaWorld-Expo'\n",
    "\n",
    "from_to = re.compile('.*from(.*)to(.*)')\n",
    "to_from = re.compile('.*to(.*)from(.*)')\n",
    "\n",
    "from_to_match = from_to.match(sentence1)\n",
    "to_from_match = to_from.match(sentence1)\n",
    "\n",
    "if from_to_match and from_to_match.groups():\n",
    "    _from = from_to_match.groups()[0]\n",
    "    _to = from_to_match.groups()[1]\n",
    "    print(\"from_to pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))\n",
    "elif to_from_match and to_from_match.groups():\n",
    "    _to = to_from_match.groups()[0]\n",
    "    _from = to_from_match.groups()[1]\n",
    "    print(\"to_from pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
